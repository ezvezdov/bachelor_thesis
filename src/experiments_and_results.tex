%!TEX root = ../main.tex

\chapter{Experiments and Results \label{chap:experiments_and_results}}

\section{Text representation models}
Table \ref{tab:evaluatinon} presents the evaluation results for the chosen text embedding models.

\subsection{Analysis of results}
\subsubsection{Czech models}
Among the evaluated Czech models, SimCSE-RetroMAE-Small demonstrated the most promising performance.
This model effectively handled both text with diacritics and diacriticless variants, outperforming other models within this category.
Notably, the remaining Czech models achieved performance below the established baseline.

The performance of Czert-B can potentially be attributed to the fact that it was not specifically fine-tuned for the task of generating text embeddings.
This highlights the importance of fine-tuning models for the specific task at hand to optimize their performance.

Regarding the Seznam models, the observed differences in results likely stem from variations in their training processes.
The SimCSE-RetroMAE-Small model emerged as the clear leader within this group, suggesting that the fine-tuning strategy employed with \ac{SimCSE} and \ac{RetroMAE} may be particularly effective for these models.

\subsubsection{Multilingual models}
The evaluation revealed particularly strong performance from all versions of \ac{mE5} and \ac{LaBSE} among the multilingual models.
Notably, \ac{mE5} models achieved the highest overall results within the tested set.
This finding suggests that the training objectives and architectures employed for these models are well-suited for capturing semantic similarity across languages, including Czech.
Confusion matrices for the \ac{mE5}\textsubscript{Large} evaluation are presented in \reffig{fig:Confusion_matrices}.

Paraphrase-Multilingual-MiniLM-L12-v2 and Paraphrase-Multilingual-MPNet-Base-v2 demonstrated strong performance with text that included diacritics.
However, their effectiveness notably decreased when processing diacriticless text.
This suggests a limitation in these models' capacity to generalize across different forms of Czech text.
To improve their performance, additional training or adaptation tailored to handle both diacritic and diacriticless variations in Czech text would be beneficial.

While \ac{mBERT} and \ac{XLM-R} did not achieve the same level of success as other models, it is important to consider the limitations of their unsupervised training methods (e.g., \ac{MLM} and \ac{NSP}) which may not be specifically optimized for the task of assessing semantic similarity.
Utilizing alternative training objectives or supervised learning approaches tailored for this task could potentially lead to improved performance from these models in the context of Czech text analysis.

\subsubsection{Monolingual models}

While all monolingual models achieved performance near the established baseline, this is a noteworthy finding considering they were not specifically trained on the Czech language.
This suggests that some level of semantic similarity can be captured between languages with inherent structural similarities, even without targeted training on the target language.
However, it is important to acknowledge that languages possess distinct vocabularies, grammatical structures, and cultural nuances.
Models trained solely on a language like English may struggle to fully grasp the intricacies of Czech text, potentially hindering their ability to achieve optimal performance in tasks involving semantic similarity assessment.

In contrast, several monolingual models, particularly the Nomic Embed models, the small version of GTE, and the large version of GTE-v1.5, exhibited performance exceeding the baseline.
This highlights the potential effectiveness of certain model architectures, even when not specifically trained on the target language.
Further investigation into the specific characteristics of these models that contribute to their success in this context might provide valuable insights for future research.

\begin{figure}[htbp]
    \centering

    \subfloat[Evaluatinon using diacritics tests.]{%
        \centering
        \includegraphics[width=0.54\textwidth,valign=t]{src/fig/pdfs/CM_QMA_FAQ50_diacritics.pdf}
        \includegraphics[width=0.54\textwidth,valign=t]{src/fig/pdfs/CM_AMA_FAQ50_diacritics.pdf}
        \label{fig:CM_diacritics}
    }

    \subfloat[Evaluatinon using diacriticless tests.]{%
        \includegraphics[width=0.54\textwidth,valign=t]{src/fig/pdfs/CM_QMA_FAQ50_diacriticless.pdf}
        \includegraphics[width=0.54\textwidth,valign=t]{src/fig/pdfs/CM_AMA_FAQ50_diacriticless.pdf}
        \label{fig:CM_diacriticless}
    }

    \caption{Confusion matrices of \ac{mE5}\textsubscript{Large} evaluated using diacritics (a) and diacriticless (b) FAQ50 subsets of UPV FAQ dataset.}
    \label{fig:Confusion_matrices}
\end{figure}

\FloatBarrier

\input{src/tables/evaluation.tex}

\FloatBarrier

\subsection{Balanced models}
To ensure the effectiveness of the evaluation process, a selection criterion was applied to the initial set of candidate models.
This criterion focused on Question Matching Accuracy and Answer Matching Accuracy for both diacritic and diacriticless models.
Models that exhibited performance below the established baseline for their respective category (diacritic or diacriticless) were excluded from further evaluation.

Additionally, models with lower performance metrics were removed if a smaller, more efficient model demonstrated comparable or superior accuracy.
This approach ensures that the final selection of models for evaluation represents a balance between effectiveness and efficiency.

\input{src/tables/models_balanced.tex}

\section{RAG Optimization}

\input{src/tables/RAG_evaluation.tex}

% \section{TODO: REMOVE Experiments structure}
% \begin{itemize}
%     \item Present the results of the evaluation for different text representations using analogy tests and confusion matrices.
%     \item Discuss the findings regarding the effectiveness of each representation model for capturing semantic relationships in technical text.
%     \item Analyze the results from the RAG evaluation, highlighting the impact of different representations and text chunk sizes on answer generation quality and CPU efficiency.
%     \item Identify the representation model that achieves a balance between factuality of answers and computational demands.
% \end{itemize}
