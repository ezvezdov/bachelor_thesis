%!TEX root = ../main.tex

\chapter{Experiments and Results\label{chap:experiments_and_results}}

\section{Text representation models}
In \reftab{tab:evaluatinon} we can see evaluation of chosen models

\subsection{Analysis of results}
\subsubsection{Czech models}
Among the evaluated Czech models, SimCSE-RetroMAE-Small demonstrated the most promising performance.
This model effectively handled both text with diacritics and diacriticless variants, outperforming other models within this category.
Notably, the remaining Czech models achieved performance below the established baseline.

The performance of Czert-B can potentially be attributed to the fact that it was not specifically fine-tuned for the task of generating text embeddings.
This highlights the importance of fine-tuning models for the specific task at hand to optimize their performance.

Regarding the Seznam models, the observed differences in results likely stem from variations in their training processes.
The SimCSE-RetroMAE-Small model emerged as the clear leader within this group, suggesting that the fine-tuning strategy employed with \ac{SimCSE} and \ac{RetroMAE} may be particularly effective for these models.

\subsubsection{Multilingual models}
Among the evaluated multilingual models, all versions of mE5 and LaBSE demonstrated particularly promising performance.
Notably, mE5 achieved the highest overall results within the tested models.
This suggests that the training objectives and architectures employed for these models might be well-suited for capturing semantic similarity across languages, including Czech.

Paraphrase-Multilingual-MiniLM-L12-v2 and Paraphrase-Multilingual-MPNet-Base-v2 exhibited strong performance when applied to text containing diacritics.
However, their performance appeared less effective when handling diacriticless text.
This observation suggests that these models might benefit from further training or adaptation specifically tailored for handling variations in Czech text with and without diacritics.

While mBERT and XLM-R did not achieve the same level of success as other models, it's important to acknowledge that the unsupervised training methods they employ (e.g. \ac{MLM} and \ac{NSP}) might not be specifically optimized for the task of assessing semantic similarity.
Alternative training objectives or supervised learning approaches tailored for this task might lead to improved performance from these models in the context of Czech text analysis.

\subsubsection{Monolingual models}

While all monolingual models achieved performance near the established baseline, this is a noteworthy finding considering they were not specifically trained on the Czech language.
This suggests that some level of semantic similarity can be captured between languages with inherent structural similarities, even without targeted training on the target language.
However, it is important to acknowledge that languages possess distinct vocabularies, grammatical structures, and cultural nuances.
Models trained solely on a language like English may struggle to fully grasp the intricacies of Czech text, potentially hindering their ability to achieve optimal performance in tasks involving semantic similarity assessment.

In contrast, several monolingual models, particularly the Nomic Embed models, the small version of GTE, and the large version of GTE-v1.5, exhibited performance exceeding the baseline.
This highlights the potential effectiveness of certain model architectures, even when not specifically trained on the target language.
Further investigation into the specific characteristics of these models that contribute to their success in this context might provide valuable insights for future research.
\FloatBarrier

\input{src/tables/evaluation.tex}

\FloatBarrier

% \newpage



\subsection{Balanced models}
To ensure the effectiveness of the evaluation process, a selection criterion was applied to the initial set of candidate models.
This criterion focused on Question Matching Accuracy and Answer Matching Accuracy for both diacritic and diacriticless models.
Models that exhibited performance below the established baseline for their respective category (diacritic or diacriticless) were excluded from further evaluation.

Additionally, models with lower performance metrics were removed if a smaller, more efficient model demonstrated comparable or superior accuracy.
This approach ensures that the final selection of models for evaluation represents a balance between effectiveness and efficiency.

\input{src/tables/models_balanced.tex}

\begin{itemize}
    \item Present the results of the evaluation for different text representations using analogy tests and confusion matrices.
    \item Discuss the findings regarding the effectiveness of each representation model for capturing semantic relationships in technical text.
    \item Analyze the results from the RAG evaluation, highlighting the impact of different representations and text chunk sizes on answer generation quality and CPU efficiency.
    \item Identify the representation model that achieves a balance between factuality of answers and computational demands.
\end{itemize}

\section{RAG Optimization}