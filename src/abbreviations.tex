%!TEX root = ../main.tex

\begin{acronym}
  \acro{CTU}[CTU]{Czech Technical University}
  \acro{API}[API]{Application Programming Interface}
  \acro{RAG}[RAG]{Retrieval-Augmented Generation}
  \acro{NLP}[NLP]{Natural Language Procession}
  \acro{STS}[STS]{Semantic Textual Similarity}
  \acro{QA}[QA]{Question Answering}
  \acro{BERT}[BERT]{Bidirectional Encoder Representations from Transformers}
  \acro{CBOW}[CBOW]{Continuous Bag-of-Words}
  \acro{GloVe}[GloVe]{Global vectors}
  \acro{ML}[ML]{Machine Learning}
  \acro{NN}[NN]{Neural Network}
  \acro{BoW}[BoW]{Bag-of-Words}
  \acro{TF-IDF}[TF-IDF]{Term Frequency-Inverse Document Frequency}
  \acro{OOV}[OOV]{Out of vocabulary}
  \acro{mBERT}[mBERT]{Multilingual Bidirectional Encoder Representations from Transformers}
  \acro{MTEB}[MTEB]{Massive Text Embedding Benchmark}
  \acro{ALBERT}[ALBERT]{A Lite BERT}
  \acro{SimCSE}[SimCSE]{Simple Contrastive Learning of Sentence Embeddings}
  \acro{RetroMAE}[RetroMAE]{Retrieval-oriented Language Models Via Masked Auto-Encoder}
  \acro{mE5}[mE5]{Multilingual E5}
  \acro{LaBSE}[LaBSE]{Language-Agnostic BERT Sentence Embedding}
  \acro{MLM}[MLM]{Masked Language Modeling}
  \acro{XLM-R}[XLM-R]{XLM-Roberta}
  \acro{NSP}[NSP]{Next Sentence Prediction}
  \acro{BGE}[BGE]{BAAI General Embeddings}
  \acro{GTE}[GTE]{General Text Embedding}
\end{acronym}

