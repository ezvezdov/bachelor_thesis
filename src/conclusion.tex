%!TEX root = ../main.tex

\chapter{Conclusion \label{chap:conclusion}}

This thesis investigated the effectiveness of transformer-based models for text representation compared to traditional methods in the context of semantic similarity assessment for Czech text.

The analysis began with a comprehensive review of traditional text representation methods.
Their architectures, underlying principles, strengths, weaknesses, and specific applications were examined.
This review established a foundation for understanding the evolution of text representation techniques.

Following this, the study shifted its focus to transformer architectures.
Here, the investigation delved into the inner workings of these models and explored their advantages over traditional methods.
The \ac{BERT} model served as a specific example, with an explanation of its training process and its strengths in capturing semantic meaning from textual data.

To ensure objective assessment of model quality for the chosen task, two relevant evaluation methods were reviewed.
These established methods provided a framework for comparing the performance of different text representation models used for semantic similarity assessment in Czech text.

Next, the investigation explored the \ac{RAG} model.
Core concepts, operational principles, and critical parameters influencing \ac{RAG}'s accuracy were examined.
This in-depth analysis proved crucial for effectively configuring and evaluating \ac{RAG} within the context of the chosen task.

Recognizing the importance of language-specific analysis, Czech was chosen as the target language.
The study incorporated both diacritic and diacritic-less text versions to account for potential variations within Czech text data.
The established UPV FAQ benchmark served as the standard for consistent and reliable evaluation.

A baseline performance metric was established using the fastText model.
This baseline provided a benchmark for comparing the performance of the transformer-based models.
Following the establishment of this baseline, a diverse selection of 15 transformer-based model groups (encompassing a total of 37 models) were chosen for further evaluation.

The \ac{RAG} evaluation process involved testing five different chunk sizes.
This exploration aimed to understand the impact of chunk size on both the factuality (accuracy) of retrieved information and computational efficiency (processing time).
The initial stage focused on selecting optimal text representation models.
We employed \ac{GTE}\textsubscript{Small} for embedding generation and GPT-3.5-turbo for answer generation.
GPT-4o was used in a separate process to assess the quality of answers generated by GPT-3.5-turbo.

After we made evaluation of the chosen models to detect best text representation models.
The evaluation results revealed that a significant portion of the transformer-based models outperformed the baseline, suggesting their promise for semantic similarity assessment in Czech text.
A detailed analysis of model performance and influencing factors identified \ac{mE5} \textsubscript{Large} as the top performer.
A confusion matrix visualized its evaluation on a specific benchmark subset.
Additionally, "balanced models" exhibiting the best performance relative to their model size were highlighted.
These included SimCSE-RetroMAE-Small, the small version of \ac{GTE}, and all sizes of \ac{mE5}, demonstrating the potential of both large and efficient models for this task.

The evaluation then proceeded to analyze the impact of chunk size on the \ac{RAG} model itself.
The first stage involved calculating the average time required for embedding generation with different chunk sizes using \ac{GTE}\textsubscript{Small}.
This analysis aimed to identify potential variations in processing time based on chunk size.
The results yielded unexpected findings, which were subsequently interpreted as a potential consequence of improved transformer model parallelism when handling larger sequences of tokens.

Following the analysis of processing time, the study investigated the impact of chunk size on model accuracy.
Based on the evaluation results, a chunk size of 4096 characters was identified as optimal for the \ac{RAG} model.


% \section{CONCLUSION STRUCTURE}
% \begin{itemize}
%     \item Summarize the key takeaways from the research, emphasizing the most effective text representation model for RAG in technical QA based on the evaluation criteria.
%     \item Briefly mention the trade-offs between factuality, CPU usage, and other factors in selecting representations for RAG.
%     \item Suggest potential future research directions, such as exploring other text representation methods or evaluating RAG performance on different datasets.
% \end{itemize}
