%!TEX root = ../main.tex

\chapter{Literature Review\label{chap:literature_review}}

\section{Traditional word embedding methods}

\subsection{Word2Vec}
Word2Vec \cite{mikolov2013efficient} is algorithm that generates word embedding using information about target word (context).
Word2Vec uses \ac{NN} and \ac{ML} techniques to generate word embedding for every word in vocabulary during training.
As \ac{NN} architecture are used \ac{CBOW} and Skip-gram, \reffig{fig:cbow_skipgram_scheme}.

\begin{figure}[h]
    \centering
    \input{src/fig/tikz/cbow_scheme.tex}
    \input{src/fig/tikz/skip_gram_scheme.tex}
    \caption{\ac{CBOW} and Skip-gram schemes respectively}
    \label{fig:cbow_skipgram_scheme}
\end{figure} 

Due to its algorithmic simplicity and efficiency, Word2Vec has established itself as a strong baseline for numerous \ac{NLP} tasks.
Compared to more recent and complex models, Word2Vec requires minimal hyperparameter tuning, making it a relatively straightforward approach.

However, it is important to acknowledge that Word2Vec has limitations.
These include its inability to capture \textbf{global information} within a document, its challenges in effectively handling \textbf{morphologically rich languages} (languages with many word variations), and its lack of awareness of the \textbf{broader context} beyond a limited window of surrounding words.

\subsection{GloVe}
\ac{GloVe} \cite{pennington2014glove} leverages the co-occurrence statistics of words within a corpus to learn vector representations.
This approach involves constructing a co-occurrence matrix, where each entry reflects the frequency of two words appearing together within a predefined window size.
This matrix essentially captures the relative importance of various word pairings.

A core principle of \ac{GloVe} lies in the notion that word vectors should effectively encode the ratios between co-occurrence probabilities of words.
By analyzing these ratios, \ac{GloVe} can identify semantic relationships between words.
This is achieved by factorizing the co-occurrence matrix into a lower-dimensional space, allowing for efficient representation and manipulation of word meanings.

To optimize the learned word embeddings, \ac{GloVe} employs a weighted least squares objective function.
This function aims to minimize the discrepancy between the dot product of two word vectors and the logarithm of their co-occurrence probability.
Through iterative adjustments of the word vectors, \ac{GloVe} converges on a solution that yields the desired word embeddings.

\subsection{FastText}
FastText \cite{bojanowski2017enriching} utilizes similar \ac{NN} architectures as word2vec, namely \ac{CBOW} and Skip-gram, but applies them to character n-grams (subwords) instead of entire words.
This decomposition allows FastText to represent a word's meaning by considering its constituent subword components.
Consequently, FastText offers advantages in two key areas:
\begin{itemize}
    \item Rare Word Embeddings: Unlike word2vec, which struggles with words appearing infrequently in the training data, FastText can construct meaningful representations for rare words. By leveraging known subwords, FastText can represent unseen words, making it particularly valuable for working with large and diverse datasets.    
    \item Handling Morphologically Rich Languages: Languages with complex morphology, where words are formed through prefixes and suffixes, often pose challenges for word2vec. FastText overcomes this limitation by capturing the shared subwords between derived words and their root forms. This allows FastText to represent the inherent relationships between words in these languages, leading to more accurate NLP tasks.
\end{itemize}
However, it's important to acknowledge that FastText also has limitations:
\begin{itemize}
    \item Context Insensitivity: Similar to word2vec, FastText embeddings do not inherently capture the order or context in which words appear within a sentence. This can be a drawback for tasks like sentiment analysis or machine translation, where word order and context are crucial for accurate interpretation.
    \item Limited Long-Range Dependency Capture: While subwords allow FastText to capture local context, it might not effectively capture long-range dependencies within sentences. This can be a disadvantage for tasks requiring analysis of complex sentence structures, where understanding the relationships between words across larger distances is important.    
\end{itemize}

\section{Transformer-based models}
Transformer models \cite{vaswani2023attention} \nocite{umarjamilai} underpin powerful NLP models like BERT.
A key advantage is their self-attention mechanism, which assigns importance to words based on context, not just position.
This enables efficient parallel processing of entire sentences.
Architecture of transformers visualized in \reffig{fig:model-arch}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{src/fig/imgs/transformer_arch.png}
    \caption{The Transformer - model architecture.}
    \label{fig:model-arch}
\end{figure}

BERT builds on transformers with pre-training on a massive text corpus.
\ac{MLM} and \ac{NSP} further enhance \ac{BERT}'s capabilities, fostering deep contextual understanding and grasp of sentence relationships.

These strengths make transformers, particularly \ac{BERT}, well-suited for \ac{NLP} tasks.
Their advantage lies in capturing contextual understanding, leading to richer text representations and superior comprehension of semantic relationships.

Furthermore, \ac{BERT} excels in transfer learning, readily adapting to various tasks (sentiment analysis, \ac{QA}) with minimal modifications.
Additionally, efficiency and speed are benefits due to parallel processing and pre-training.

Transformer models effectiveness is validated by state-of-the-art performance across \ac{NLP} benchmarks.
Finally, \ac{BERT}'s robustness allows it to handle nuances in text without significant performance degradation.

\section{Methods of text representations evaluation}

\subsection{Analogy Tests}

Analogy tests, as demonstrated in the seminal word2vec paper \cite{mikolov2013efficient}, are a widely used method for assessing the quality of text representations, particularly word embeddings.
These tests evaluate whether the semantic relationships between words are effectively captured and preserved within the vector space employed by the model.

A typical analogy test question follows the format "A is to B as C is to D," where A, B, C, and D represent words.
For instance, the question "man is to king as woman is to queen" probes the model's understanding of gender relations.
If the word embeddings are of high quality, performing the vector operation vector(king) - vector(man) + vector(woman) should result in a vector that closely resembles vector(queen).
This outcome indicates that the model has successfully learned the analogous relationship between "man" and "king" and "woman" and "queen."

\subsection{Confusion matrix}

Confusion matrices are a widely used tool for evaluating classification algorithms, and they can be adapted to assess text representations in tasks such as word sense disambiguation, part-of-speech tagging, or sentiment analysis.
A confusion matrix is a table that describes the performance of a classification model by comparing predicted and actual labels.

\section{\acf{RAG}}
\ac{RAG} \cite{lewis2021retrievalaugmented} \nocite{umarjamilai} is an advanced \ac{NLP} framework that combines the strengths of retrieval-based and generation-based models to produce high-quality, contextually relevant text based on provided document (web-page etc.), instruction and query (question). 

\subsection{Architecture of \ac{RAG}}
The \ac{RAG} algorithm leverages a two-stage approach for answer generation: retrieval and generation.
Both stages rely heavily on the chosen text representation technique.

\begin{itemize}
  \item \textbf{Retrieval}:
    In the initial phase, the algorithm extracts relevant information from the document and splits it into manageable chunks.
    These chunks are then fed into text representation models, which convert them into a format suitable for efficient retrieval.
    This process results in encoded representations of the information, which are then stored within a vector database.
    During the retrieval phase, \ac{RAG} utilizes the same text representation model to encode the user's query (question).
    Subsequently, it searches the vector database and identifies the top-$K$ most relevant passages based on their encoded representations.
  \item \textbf{Generation}:
    The $K$ retrieved passages identified in the information retrieval phase serve as crucial contextual information for the \ac{LLM} within the \ac{RAG} system.
    By providing this context alongside the user's question and any additional instructions, the \ac{LLM} is empowered to generate a comprehensive and informative answer.
\end{itemize}

Architecture of the \ac{RAG} algorithm is shown on the \reffig{fig:RAG_scheme}

\begin{figure}[h]
  \centering
  \input{src/fig/tikz/RAG_scheme.tex}
  \caption{Retrieval-Augmented Generation architecture.}
  \label{fig:RAG_scheme}
\end{figure}

\subsection{Factors Influencing \ac{RAG} Performance}
The performance of \ac{RAG} systems can be influenced by several key factors.
Two important aspects are:

\begin{itemize}
    \item \textbf{Embedding Model Selection}:
        Previous work \cite{joshi2024RAGemb} suggests that the choice of embedding model significantly impacts \ac{RAG} performance.
        Different embedding models offer varying strengths in capturing semantic relationships within text data.
        Selecting the most suitable model depends on the specific task and dataset.
    \item \textbf{Document Chunking Size}:
        Another factor influencing \ac{RAG} performance is the size of the document chunks used for retrieval \cite{theja2023RAGchunk}.
        Splitting documents into smaller chunks can potentially improve retrieval efficiency.
        However, excessively small chunks may lead to a loss of context and hinder the \ac{RAG} system's ability to generate coherent and relevant text.
        Finding the optimal chunking size requires careful consideration of the task and available computational resources.    
\end{itemize}




% Literature Review Structure
% \begin{itemize}
%     \item Discuss traditional word embedding methods like FastText and their limitations.
%     \item Explain the concept of transformer-based models like BERT and their advantages for text representation.
%     \item Review related work on RAG algorithms and their dependence on effective text representations.Discuss existing research on evaluating text representations using analogy tests and confusion matrices.
%     \item Briefly mention the UPV corpus set as the chosen evaluation benchmark.
% \end{itemize}