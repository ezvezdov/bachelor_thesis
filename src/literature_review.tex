%!TEX root = ../main.tex

\chapter{Literature Review\label{chap:literature_review}}

\section{Traditional word embedding methods}

\subsection{Word2Vec}
Word2Vec \cite{word2vec} is algorithm that generates word embedding using information about target word (context).
Word2Vec uses \ac{NN} and \ac{ML} techniques to generate word embedding for every word in vocabulary during training.
As \ac{NN} architecture are used \ac{CBOW} and Skip-gram, \reffig{fig:cbow_skipgram_scheme}.

\begin{figure}[h]
    \centering
    \input{src/fig/tikz/cbow_scheme.tex}
    \input{src/fig/tikz/skip_gram_scheme.tex}
    \caption{\ac{CBOW} and Skip-gram schemes respectively}
    \label{fig:cbow_skipgram_scheme}
\end{figure} 

Due to its algorithmic simplicity and efficiency, Word2Vec has established itself as a strong baseline for numerous \ac{NLP} tasks.
Compared to more recent and complex models, Word2Vec requires minimal hyperparameter tuning, making it a relatively straightforward approach.

However, it is important to acknowledge that Word2Vec has limitations.
These include its inability to capture \textbf{global information} within a document, its challenges in effectively handling \textbf{morphologically rich languages} (languages with many word variations), and its lack of awareness of the \textbf{broader context} beyond a limited window of surrounding words.

\subsection{GloVe}
\ac{GloVe} \cite{glove} leverages the co-occurrence statistics of words within a corpus to learn vector representations.
This approach involves constructing a co-occurrence matrix, where each entry reflects the frequency of two words appearing together within a predefined window size.
This matrix essentially captures the relative importance of various word pairings.

A core principle of \ac{GloVe} lies in the notion that word vectors should effectively encode the ratios between co-occurrence probabilities of words.
By analyzing these ratios, \ac{GloVe} can identify semantic relationships between words.
This is achieved by factorizing the co-occurrence matrix into a lower-dimensional space, allowing for efficient representation and manipulation of word meanings.

To optimize the learned word embeddings, \ac{GloVe} employs a weighted least squares objective function.
This function aims to minimize the discrepancy between the dot product of two word vectors and the logarithm of their co-occurrence probability.
Through iterative adjustments of the word vectors, \ac{GloVe} converges on a solution that yields the desired word embeddings.


\subsection{FastText}
FastText \cite{fasttext} utilizes similar \ac{NN} architectures as word2vec, namely \ac{CBOW} and Skip-gram, but applies them to character n-grams (subwords) instead of entire words.
This decomposition allows FastText to represent a word's meaning by considering its constituent subword components.
Consequently, FastText offers advantages in two key areas:
\begin{itemize}
    \item Rare Word Embeddings: Unlike word2vec, which struggles with words appearing infrequently in the training data, FastText can construct meaningful representations for rare words. By leveraging known subwords, FastText can represent unseen words, making it particularly valuable for working with large and diverse datasets.    
    \item Handling Morphologically Rich Languages: Languages with complex morphology, where words are formed through prefixes and suffixes, often pose challenges for word2vec. FastText overcomes this limitation by capturing the shared subwords between derived words and their root forms. This allows FastText to represent the inherent relationships between words in these languages, leading to more accurate NLP tasks.
\end{itemize}
However, it's important to acknowledge that FastText also has limitations:
\begin{itemize}
    \item Context Insensitivity: Similar to word2vec, FastText embeddings do not inherently capture the order or context in which words appear within a sentence. This can be a drawback for tasks like sentiment analysis or machine translation, where word order and context are crucial for accurate interpretation.
    \item Limited Long-Range Dependency Capture: While subwords allow FastText to capture local context, it might not effectively capture long-range dependencies within sentences. This can be a disadvantage for tasks requiring analysis of complex sentence structures, where understanding the relationships between words across larger distances is important.    
\end{itemize}

\section{Transformer-based models}
Transformer models \cite{vaswani2023attention} underpin powerful NLP models like BERT.
A key advantage is their self-attention mechanism, which assigns importance to words based on context, not just position.
This enables efficient parallel processing of entire sentences.
Architecture of transformers visualized in \reffig{fig:model-arch}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{src/fig/imgs/transformer_arch.png}
    \caption{The Transformer - model architecture.}
    \label{fig:model-arch}
\end{figure}

BERT builds on transformers with pre-training on a massive text corpus.
\ac{MLM} and \ac{NSP} further enhance \ac{BERT}'s capabilities, fostering deep contextual understanding and grasp of sentence relationships.

These strengths make transformers, particularly \ac{BERT}, well-suited for \ac{NLP} tasks.
Their advantage lies in capturing contextual understanding, leading to richer text representations and superior comprehension of semantic relationships.

Furthermore, \ac{BERT} excels in transfer learning, readily adapting to various tasks (sentiment analysis, \ac{QA}) with minimal modifications.
Additionally, efficiency and speed are benefits due to parallel processing and pre-training.

Transformer models effectiveness is validated by state-of-the-art performance across \ac{NLP} benchmarks.
Finally, \ac{BERT}'s robustness allows it to handle nuances in text without significant performance degradation.

\section{TODO: REMOVE, STRUCTURE}
\begin{itemize}
    \item Discuss traditional word embedding methods like FastText and their limitations.
    \item Explain the concept of transformer-based models like BERT and their advantages for text representation.
    \item Review related work on RAG algorithms and their dependence on effective text representations.Discuss existing research on evaluating text representations using analogy tests and confusion matrices.
    \item Briefly mention the UPV corpus set as the chosen evaluation benchmark.
\end{itemize}