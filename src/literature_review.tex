%!TEX root = ../main.tex

\chapter{Literature Review\label{chap:literature_review}}

\section{Traditional word embedding methods}

\subsection{Word2Vec \cite{word2vec}}
Word2Vec is algorithm that generates word embedding using information about target word (context).
Word2Vec uses \ac{NN} and \ac{ML} techniques to generate word embedding for every word in vocabulary during training.
As \ac{NN} architecture are used \ac{CBOW} and Skip-gram, \reffig{fig:cbow_skipgram_scheme}.

\begin{figure}[h]
    \centering
    \input{src/fig/tikz/cbow_scheme.tex}
    \input{src/fig/tikz/skip_gram_scheme.tex}
    \caption{\ac{CBOW} and Skip-gram schemes respectively}
    \label{fig:cbow_skipgram_scheme}
\end{figure} 

Due to its algorithmic simplicity and efficiency, Word2Vec has established itself as a strong baseline for numerous \ac{NLP} tasks.
Compared to more recent and complex models, Word2Vec requires minimal hyperparameter tuning, making it a relatively straightforward approach.

However, it is important to acknowledge that Word2Vec has limitations.
These include its inability to capture \textbf{global information} within a document, its challenges in effectively handling \textbf{morphologically rich languages} (languages with many word variations), and its lack of awareness of the \textbf{broader context} beyond a limited window of surrounding words.

\subsection{\ac{GloVe} \cite{glove}}
\ac{GloVe} leverages the co-occurrence statistics of words within a corpus to learn vector representations.
This approach involves constructing a co-occurrence matrix, where each entry reflects the frequency of two words appearing together within a predefined window size.
This matrix essentially captures the relative importance of various word pairings.

A core principle of \ac{GloVe} lies in the notion that word vectors should effectively encode the ratios between co-occurrence probabilities of words.
By analyzing these ratios, \ac{GloVe} can identify semantic relationships between words.
This is achieved by factorizing the co-occurrence matrix into a lower-dimensional space, allowing for efficient representation and manipulation of word meanings.

To optimize the learned word embeddings, \ac{GloVe} employs a weighted least squares objective function.
This function aims to minimize the discrepancy between the dot product of two word vectors and the logarithm of their co-occurrence probability.
Through iterative adjustments of the word vectors, \ac{GloVe} converges on a solution that yields the desired word embeddings.


\subsection{FastText \cite{fasttext}}


\section{Transformer-based models}


\begin{itemize}
    \item Discuss traditional word embedding methods like FastText and their limitations.
    \item Explain the concept of transformer-based models like BERT and their advantages for text representation.
    \item Review related work on RAG algorithms and their dependence on effective text representations.Discuss existing research on evaluating text representations using analogy tests and confusion matrices.
    \item Briefly mention the UPV corpus set as the chosen evaluation benchmark.
\end{itemize}