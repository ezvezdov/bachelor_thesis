\reftab{tab:parameters} presents a comparison of the architectural characteristics of the evaluated text embedding models.
This table includes the following parameters for each model

\begin{itemize}
    \item \textbf{Number of Layers ($L$)}:
        This refers to the number of encoder or decoder layers stacked within the model architecture.
        A higher number of layers typically indicates a more complex model with greater capacity to learn complex relationships within the data.
    \item \textbf{Number of Hidden States ($H_m$)}:
        This represents the dimensionality of the internal representations processed by each layer within the model.
        A larger number of hidden states allows the model to capture a richer set of features from the input data.
    \item \textbf{Dimension of Feed-Forward Layer ($H_{ff}$)}:
        This parameter specifies the dimensionality of the hidden layer within the feed-forward sub-layer of each transformer encoder block.
        The feed-forward sub-layer allows the model to learn non-linear relationships between input features.
    \item \textbf{Number of Attention Heads ($A$)}:
        This refers to the number of parallel attention mechanisms employed within each encoder or decoder layer.
        A higher number of attention heads allows the model to focus on different aspects of the input data simultaneously.
    \item \textbf{Dimension of Output Embedding ($D$)}:
        This specifies the dimensionality of the final vector representation generated by the model for each input text sequence.
    \item \textbf{Maximum Sequence Length ($T_{max}$)}:
        This parameter indicates the maximum number of tokens a model can process within a single input sequence.
        Models with a larger $T_{max}$ can handle longer text inputs without requiring truncation.
    \item \textbf{Vocabulary Size ($V$)}:
        This represents the total number of unique tokens the model's vocabulary encompasses.
    \item \textbf{Total Number of Parameters ($N_p$)}:
        This denotes the total number of trainable parameters within the model.
        A larger number of parameters typically indicates a more complex model with greater capacity, but also higher computational demands.
\end{itemize}


For Transformer encoders, the number of parameters can be approximated by Eq.~\refeq{eq:model_params}.
\begin{equation}
    \label{eq:model_params}
    N_p \approx 4LH_m^2 + 2LH_m H_{ff} + VH_m
\end{equation}


\input{src/tables/models_detailed.tex}