\begin{table*}[t!]
    \centering
    \begin{tabular}{lcccccccc}
      \toprule
      % \hline
      \textbf{Model} & $L$ & $H_m$ & $H_{ff}$ & $A$ & $D$ & $V$ & \textbf{\#params} \\
      \midrule
      Czert-B & 12 & 768 & 3072 & 12 & 768 & 31K & 110M \\
      retromae-small-cs & 12 & 256 & 1024 & 4 & 256 & 58K & 24M \\
      dist-mpnet-paracrawl-cs-en & 12 & 256 & 1024 & 4 & 256 & 58K & 24M \\
      dist-mpnet-czeng-cs-en & 12 & 256 & 1024 & 4 & 256 & 58K & 24M \\
      simcse-retromae-small-cs & 12 & 256 & 1024 & 4 & 256 & 58K & 24M \\
      simcse-dist-mpnet-paracrawl-cs-en & 12 & 256 & 1024 & 4 & 256 & 58K & 24M \\
      simcse-dist-mpnet-czeng-cs-en & 12 & 256 & 1024 & 4 & 256 & 58K & 24M \\
      simcse-small-e-czech & 12 & 256 & 1024 & 4 & 256 & 31K & 13M \\
      mBERT & 12 & 768 & 3072 & 12 & 768 & 120K & 178M \\
      mE5\textsubscript{Small} & 12 & 384 & 1536 & 12 & 384 & 250K & 118M \\
      mE5\textsubscript{Base} & 12 & 768 & 3072 & 12 & 768 & 250K & 278M \\
      mE5\textsubscript{Large} & 24 & 1024 & 4096 & 16 & 1024 & 250K & 560M \\
      LaBSE & 12 & 768 & 3072 & 12 & 768 & 502K & 471M \\
      XLM-R\textsubscript{Base} & 12 & 768 & 3072 & 12 & 768 & 250K & 279M \\
      XLM-R\textsubscript{Large} & 24 & 1024 & 4096 & 16 & 1024 & 250K & 560M \\
      distiluse-base-multilingual-cased-v2 & 6 & 768 & 3072 & 12 & 512 & 120K & 135M \\
      paraphrase-multilingual-MiniLM-L12-v2 & 12 & 384 & 1536 & 12 & 512 & 250K & 118M \\
      paraphrase-multilingual-mpnet-base-v2 & 12 & 768 & 3072 & 12 & 514 & 250K & 278M \\
      UAE\textsubscript{Large} v1 & 24 & 1024 & 4096 & 16 & 1024 & 31K & 335M \\
      GTE\textsubscript{Small} & 12 & 384 & 1536 & 12 & 384 & 31K & 33M \\
      GTE\textsubscript{Base} & 12 & 768 & 3072 & 12 & 768 & 31K & 109M \\
      GTE\textsubscript{Large} & 24 & 1024 & 4096 & 16 & 1024 & 31K & 335M \\
      GTE\textsubscript{Base} v1.5 & 12 & 768 & 3072 & 12 & 768 & 31K & 137M \\
      GTE\textsubscript{Large} v1.5 & 24 & 1024 & 4096 & 16 & 1024 & 31K & 434M \\
      BGE\textsubscript{Small} v1.5 & 12 & 384 & 1536 & 12 & 384 & 31K & 33M \\
      BGE\textsubscript{Base} v1.5 & 12 & 768 & 3072 & 12 & 768 & 31K & 109M \\
      BGE\textsubscript{Large} v1.5 & 24 & 1024 & 4096 & 16 & 1024 & 31K & 335M \\
      TaylorAI/BGE\textsubscript{Micro} v2 & 3 & 384 & 1536 & 12 & 512 & 31K & 17M \\
      TaylorAI/GTE\textsubscript{Tiny} & 6 & 384 & 1536 & 12 & 384 & 31K & 23M \\
      
      \bottomrule
    \end{tabular}
    \caption{\textbf{Details on model sizes.}
            We show the number of layers $L$, the number of hidden states of the model $H_{m}$, the dimension of the feed-forward layer $H_{ff}$, the number of attention heads $A$, the dimension of output embedding $D$, the size of the vocabulary $V$ and the total number of parameters \#params. 
            For Transformer encoders, the number of parameters can be approximated by $4LH_m^2 + 2LH_m H_{ff} + VH_m$.
            While this table gives more hindsight on the difference of capacity of each model, note it does not highlight other critical differences between the models.}
    \label{tab:parameters}
  \end{table*}
  