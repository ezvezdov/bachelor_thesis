%!TEX root = ../main.tex

\chapter{Methodology\label{chap:methodology}}

\section{Evaluation process}

This work specifically targets the evaluation of word, sentence, and paragraph representation methods on datasets in the Czech language.
This focus on Czech allows for a deeper understanding of how these methods perform in a language with specific characteristics, such as a rich inflectional morphology and the presence of diacritics.

\subsection{Text Data Preparation}
In certain foreign languages, a common issue arises when individuals incorrectly write words by omitting diacritics or altering letters, \reffig{fig:diacritics_diacriticless}.
This problem is prevalent in social media, chatbots, and other informal written communications.
As a result, embedding models face challenges in comprehending text without diacritics (hereinafter diacriticless). A potential solution involves adapting data representation to accommodate both formal and informal styles of writing.

\begin{figure}[h]
  \centering
  \input{src/fig/tikz/diacritics_diacriticless.tex}
  \caption{Usual changes in informal czech texts.}
  \label{fig:diacritics_diacriticless}
\end{figure} 

This study will employ two distinct text representations: text with diacritics and text without diacritics.
To ensure optimal evaluation, the diacritic text will be assessed using datasets that preserve these diacritics, while the diacriticless text will be evaluated using datasets that lack diacritics.

As detailed in \reflst{lst:diacriticless.sh}, this script is used for creating diacriticless versions of the datasets.

\begin{lstlisting}[language=bash,basicstyle=\small\ttfamily, frame=single, caption={Script for removing diacritics using Unix utilities}, captionpos=b, label={lst:diacriticless.sh},backgroundcolor=\color{light-gray}] 
  sed 's/.*/\L&/' "$1" | iconv -f utf-8 -t ascii//TRANSLIT > diacriticless/"$1"
\end{lstlisting}

\subsection{Datasets}

\subsubsection{UPV FAQ}

Dataset is comprised of frequently asked questions (FAQs) and their answers from the Industrial Property Office of the Czech Republic website, organized into four distinct groups, as detailed in \reftab{table:UPV_FAQ_info}.

This work will evaluate two key metrics using UPV FAQ dataset
\begin{itemize}
  \item \textbf{Question matching accuracy}: This metric involves calculating the cosine similarity between all possible question pairs within a dataset.
                                            A question is considered successfully matched if its second-highest cosine similarity score corresponds to another question belonging to the same class (i.e., the question with the highest similarity is likely the same question itself). 
                                            The overall question matching accuracy is then computed as the ratio of successfully matched questions to the total number of question pairs evaluated.  
  \item \textbf{Answer matching accuracy}: This metric assesses the system's ability to identify the correct answer for a given question.
                                            The system accomplishes this by directly comparing the question with pre-generated answer embeddings.
                                            By evaluating the similarity between the question and each answer embedding, the system classifies the question as corresponding to the answer with the highest similarity score.
                                            The overall answer matching accuracy is then calculated as the proportion of questions for which the system correctly identifies the corresponding answer.  
\end{itemize}

\begin{table}[!ht]
  \centering
  \begin{tabular}{ |p{2cm}||p{3cm}| }
    \hline
    Tests set & Number of tests \\
    \hline
    FAQv5     & 2054            \\
    FAQ50     & 561             \\
    FAQ76     & 2025            \\
    FAQ76v2   & 1965            \\
    \hline
    \hline
    All       & 6605            \\
    \hline
  \end{tabular}
  \caption{Information about UPV FAQ tests} \label{table:UPV_FAQ_info}
\end{table}


\subsection{Baseline}
Due to the inherent morphological richness of the Czech language, this study adopts \textbf{FastText} as the baseline word embedding method.
This decision is motivated by FastText's ability to effectively capture morphological variations within words, a characteristic that has been shown to be advantageous for languages like Czech.
While other techniques like Word2Vec and \ac{GloVe} have been explored for word embedding generation, they have demonstrated lower performance in this context.
The FastText word embedding model will be trained using the fastText.cc library \footnote{fastText.cc library website: \url{https://fasttext.cc/}}

\subsubsection{Training parameters}
\begin{itemize}
  \item Architecture: \ac{CBOW} 
  \item Vector dimensionality: 300
  \item Loss function: Negative sampling loss
  \item Dictionary threshold (the frequency of the word to be included in the dictionary): 130
\end{itemize}

\subsubsection{Training data}

FastText model is trained on a segment of a preprocessed Common Crawl repository, which encompasses raw data from web pages, as well as metadata and text extractions.
Given the nature of this dataset, it is expected to include misspellings and text lacking diacritics.

\begin{enumerate}
  \item Eliminating duplicate entries
  \item Filtering out lines containing fewer than 9 characters
  \item Excluding URLs
  \item Breaking lines into individual sentences
  \item Converting all text to lowercase
  \item Removing lines containing words exceeding 30 characters
  \item Excluding HTML tags with less than 100 characters
  \item Removing lines surpassing 500 characters
  \item Omitting words longer than 21 characters
\end{enumerate}

Processed corpus contains 3.87 billion words.

\subsubsection{Model summary}

The FastText models trained in this study exhibit differences in vocabulary size.
The diacritic model possesses a dictionary of approximately 800,000 words, while the diacriticless model contains roughly 762,000 words.
This discrepancy reflects the inherent reduction in word count due to the removal of diacritics in the diacriticless dataset.

These vocabulary sizes directly influence the number of trainable parameters within each model.
The number of parameters ($N$) can be calculated using the formula~\refeq{eq:cbow_parameters}

\begin{align} \label{eq:cbow_parameters}
  & N = V_{\text{size}}d + d \\
  \text{where} \quad & V_{\text{size}} \text{ is model vocabulary size}  \notag \\
                      & d \quad \text{is chosen dimensionality of word embeddings} \notag
\end{align}
  
Applying this formula to the vocabulary sizes of our models:
\begin{itemize}
  \item The diacritics model possesses approximately 240 million parameters.
  \item The diacriticless model has approximately 229 million parameters.
\end{itemize}

\subsection{Chosen transformer models}
To ensure a comprehensive evaluation, a curated selection of embedding models will be utilized.
This selection encompasses three distinct categories:

\begin{enumerate}
  \item \textbf{Existing Czech Embedding Models}: This category incorporates established Czech embedding models developed within the Czech NLP community. Their inclusion allows for a focused analysis of how these models perform specifically for the Czech language.
  \item \textbf{Multilingual Models from the \ac{MTEB}}: The evaluation will leverage highly regarded multilingual models readily available through the \ac{MTEB}. This inclusion enables an assessment of how these models generalize to the Czech language, providing insights into their adaptability across languages.
  \item \textbf{Popular Monolingual Models from the \ac{MTEB} (rank is lower than 100)}: In addition to multilingual models, this selection will also include well-regarded monolingual models (models trained on a single language) from the \ac{MTEB}. This allows for a comparative analysis of how these models, potentially trained on English or other high-resource languages, perform on the Czech dataset.
\end{enumerate}

All models should be open-source. And sentence-bert. <1B. With information about training process.
If exisit two version of models - monolingual (no czech lang) and multilingual, we evaluate only multilingual

% Czech models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Czert-B \cite{czert}}

The Czert model is a set of Czech \ac{BERT}-like language representation models developed specifically to enhance performance in processing the Czech language.
These models leverage the \ac{BERT} and \ac{ALBERT} \cite{ALBERT} architectures and are designed to outperform multilingual models by training exclusively on Czech data.
The training set includes a comprehensive corpus of Czech texts, such as Wikipedia articles, news, and other texts, accumulating to around 36GB of data.

There are 2 variants of the Czert model, Czert-A and Czert-B. Unfortunately Czert-A model is not available, so we will test only Czert-B model.
Czert-B model is based on the traditional \ac{BERT} architecture (110M parameters).
Model are pre-trained from scratch using \ac{MLM} and \ac{NSP} tasks. However, a slight modification is made to the \ac{NSP} task to adapt it better to the Czech language corpus structure.


\subsubsection{Seznam's models \cite{seznam-models}}
This study leverages a group of compact word embedding models specifically designed for the Czech language.
These models were developed by the Seznam research group with a focus on efficient word representation generation.

\begin{itemize}
  \item RetroMAE-Small: \ac{BERT}-small model trained with the \ac{RetroMAE} objective \cite{RetroMAE} on a custom Czech corpus.
  \item Dist-MPNet-ParaCrawl: \ac{BERT}-small model distilled from the \textit{sentence-transformers/all-mpnet-base-v2} \footnote{\label{hf_all-mpnet-base-v2} Model on the huggingface website: \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}} model, using parallel cs-en dataset  \cite{ParaCrawl} for training.
  \item Dist-MPNet-CzEng: \ac{BERT}-small model distilled from the \textit{sentence-transformers/all-mpnet-base-v2} \footref{hf_all-mpnet-base-v2} . model, using parallel cs-en dataset Czeng \cite{CzEng_dataset} for training.
  \item SimCSE-RetroMAE-Small: The \ac{RetroMAE}-Small model fine-tuned with the \ac{SimCSE} \cite{SimCSE}.
  \item SimCSE-Dist-MPNet-ParaCrawl: The Dist-MPNet-ParaCrawl model fine-tuned with the \ac{SimCSE}.
  \item SimCSE-Dist-MPNet-CzEng: The Dist-MPNet-CzEng fine-tuned with the \ac{SimCSE}.
  \item SimCSE-Small-E-Czech: Czech ELECTRA model \cite{small-e-czech} fine-tuned with the \ac{SimCSE} objective to enhance sentence embeddings.
\end{itemize}

The developed models are about eight times smaller and five times faster than conventional base-sized models, making them suitable for real-time applications where computational efficiency is critical.
The models are trained using techniques like pre-training, knowledge distillation, and unsupervised contrastive fine-tuning to adapt to the limited availability of labeled Czech data.


% Multilingual models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{mBERT}
\ac{mBERT} \cite{bert_mbert} leverages the same transformer-based architecture as the original \ac{BERT} model but boasts an increased number of parameters (178M) to enhance its capabilities.
A key distinction of \ac{mBERT} lies in its ability to understand and process text data across multiple languages.
To achieve this multilingual proficiency, \ac{mBERT} is trained on a massive dataset sourced from Wikipedia entries in 104 different languages.
This corpus is meticulously constructed to ensure balanced representation, meaning each language is included regardless of the size or depth of its corresponding Wikipedia.
This approach ensures that even languages with limited resources are adequately represented within the training data, fostering better performance for these languages.

\subsubsection{\ac{mE5} \cite{mE5}}
\ac{mE5} models by Microsoft are advanced text embedding models designed to operate across multiple languages based on English-only E5 models \cite{E5}.
These models are available in three variants — small, base, and large — catering to different computational efficiency and performance needs.
The \ac{mE5} models are trained using a two-phase approach.
The first phase involves weakly-supervised contrastive pre-training on about 1 billion text pairs sourced from diverse multilingual corpora (Wikipedia, mC4, Multilingual CC News, Reddit, etc.).
The second phase is supervised fine-tuning on approximately 1.6 million data points from high-quality labeled datasets (MS MARCO \cite{ms_marco_dataset}, Natural Questions (NQ) \cite{NQ_dataset}, TriviaQA \cite{TriviaQA}, SQuAD \cite{SQuAD}, etc.).

\subsubsection{LaBSE \cite{LaBSE}}
\ac{LaBSE} model, developed by Google, is a state-of-the-art model for generating sentence embeddings that are effective across 109 languages.
It leverages the transformer architecture and is trained on both monolingual (from sources like CommonCrawl and Wikipedia) and bilingual data (mined from web pages).
LaBSE utilizes a dual-encoder structure with \ac{BERT}-based encoding modules.
This setup enables the efficient processing of text pairs in multiple languages.

\subsubsection{XLM-R \cite{xlm-roberta} }
The \ac{XLM-R} model is a significant advancement in unsupervised cross-lingual representation learning, introduced by Facebook AI.
It is specifically designed to improve performance across a wide range of cross-lingual tasks.
\ac{XLM-R} is pre-trained on a dataset dubbed 'CC-100', derived from Common Crawl, covering about 2.5 terabytes of text across 100 languages.
This dataset is significantly larger than the ones used by its predecessors, offering a broader and more diverse linguistic foundation.

\subsubsection{SentenceTransformers multilingual models \cite{reimers-2019-sentence-bert}}

\begin{itemize}
  \item distiluse-base-multilingual-cased-v2: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder \cite{yang2019multilingual}. This version supports 50+ languages including Czech.
  \item paraphrase-multilingual-MiniLM-L12-v2: Multilingual version of paraphrase-MiniLM-L12-v2, trained on parallel data for 50+ languages including Czech.
  \item paraphrase-multilingual-mpnet-base-v2: Multilingual version of paraphrase-mpnet-base-v2, trained on parallel data for 50+ languages including Czech.
\end{itemize}


% Monolingual models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{UAE-Large-V1 \cite{UAE-Large-V1}}
The UAE-Large-V1 model focuses on enhancing short and long \ac{STS} tasks through a novel angle-optimized text embedding approach that works by dividing text embeddings into real and imaginary components in a complex space.
This model is designed to address the challenges posed by the saturation zones of the cosine function, which can impede learning by causing vanishing gradients.

The model is trained using a hybrid objective that combines cosine similarity, in-batch negatives, and angle differences in complex space.
This approach helps overcome the limitations of traditional cosine similarity measures by ensuring better gradient flow during training.
Training dataset includes around 21K samples and is specifically designed to evaluate \ac{STS} performance on long texts, which are common in real-world applications.




\subsubsection{snowflake-arctic-embed}
This study investigates the use of snowflake-arctic-embed, a suite of text embedding models optimized for retrieval tasks. The suite offers multiple model sizes (xs, s, m, m-long, l) for flexibility.

Snowflake-arctic-embed leverages a two-stage training pipeline
\begin{enumerate}
  \item Pretraining on large batches of query-document pairs with dynamically generated negative examplesusing dataset with 400M samples combining public data and proprietary web search data.
  \item Fine-tuning the model on a smaller dataset (1M samples) consisting of query-positive document-negative document triplets to improve retrieval accuracy.
\end{enumerate}








\subsubsection{jina-embeddings}
Only jina-embeddings-v2, because it's better in \ac{MTEB}


\subsubsection{nomic-embed-text}
nomic-embed-text-v1
nomic-embed-text-v1.5


\subsubsection{gte}

gte-large-en-v1.5
gte-base-en-v1.5
gte-large
gte-base
gte-small
gte-tiny

\subsubsection{BGE}
The BGE models (BAAI General Embeddings) are part of the comprehensive C-Pack resources aimed at advancing the field of general Chinese text embeddings.
Here's a brief overview of their architecture and training process:

bge-large-en-v1.5
bge-base-en-v1.5
bge-small-en-v1.5
bge-micro-v2

\subsection{mxbai-embed-large-v1}









\begin{itemize}
    \item Describe the evaluation process for different text representations.
    \item \begin{itemize}
        \item{Specify the chosen word, sentence, and paragraph representation models (e.g., FastText, BERT variants).}
        \item{Explain the usage of analogy tests and confusion matrices for evaluation.}
        \item{Detail the selection process for the UPV corpus set and its suitability for technical QA tasks.}
      \end{itemize}
    \item Outline the second part of the study focusing on RAG for technical QA.
    \item \begin{itemize}
        \item{Explain the RAG algorithm and its reliance on text representations.}
        \item{Describe the evaluation approach for selecting optimal representations for RAG.}
        \item{Mention the factors considered during evaluation, such as embedding efficiency, text chunk size, and factuality of generated answers.}
      \end{itemize}
\end{itemize}









