%!TEX root = ../main.tex

\chapter{Methodology\label{chap:methodology}}

\section{Evaluation process}

This work specifically targets the evaluation of word, sentence, and paragraph representation methods on datasets in the Czech language.
This focus on Czech allows for a deeper understanding of how these methods perform in a language with specific characteristics, such as a rich inflectional morphology and the presence of diacritics.

\subsection{Text Data Preparation}
In certain foreign languages, a common issue arises when individuals incorrectly write words by omitting diacritics or altering letters, \reffig{fig:diacritics_diacriticless}.
This problem is prevalent in social media, chatbots, and other informal written communications.
As a result, embedding models face challenges in comprehending text without diacritics (hereinafter diacriticless). A potential solution involves adapting data representation to accommodate both formal and informal styles of writing.

\begin{figure}[h]
  \centering
  \input{src/fig/tikz/diacritics_diacriticless.tex}
  \caption{Usual changes in informal czech texts.}
  \label{fig:diacritics_diacriticless}
\end{figure} 

This study will employ two distinct text representations: text with diacritics and text without diacritics.
To ensure optimal evaluation, the diacritic text will be assessed using datasets that preserve these diacritics, while the diacriticless text will be evaluated using datasets that lack diacritics.

As detailed in \reflst{lst:diacriticless.sh}, this script is used for creating diacriticless versions of the datasets.

\begin{lstlisting}[language=bash,basicstyle=\small\ttfamily, frame=single, caption={Script for removing diacritics using Unix utilities}, captionpos=b, label={lst:diacriticless.sh},backgroundcolor=\color{light-gray}] 
  sed 's/.*/\L&/' "$1" | iconv -f utf-8 -t ascii//TRANSLIT > diacriticless/"$1"
\end{lstlisting}

\subsection{Evaluation  benchmark}

\subsubsection{UPV FAQ}

Dataset is comprised of frequently asked questions (FAQs) and their answers from the Industrial Property Office of the Czech Republic website, organized into four distinct groups, as detailed in \reftab{table:UPV_FAQ_info}.

This work will evaluate two key metrics using UPV FAQ dataset
\begin{itemize}
  \item \textbf{Question matching accuracy}: This metric involves calculating the cosine similarity between all possible question pairs within a dataset.
                                            A question is considered successfully matched if its second-highest cosine similarity score corresponds to another question belonging to the same class (i.e., the question with the highest similarity is likely the same question itself). 
                                            The overall question matching accuracy is then computed as the ratio of successfully matched questions to the total number of question pairs evaluated.  
  \item \textbf{Answer matching accuracy}: This metric assesses the system's ability to identify the correct answer for a given question.
                                            The system accomplishes this by directly comparing the question with pre-generated answer embeddings.
                                            By evaluating the similarity between the question and each answer embedding, the system classifies the question as corresponding to the answer with the highest similarity score.
                                            The overall answer matching accuracy is then calculated as the proportion of questions for which the system correctly identifies the corresponding answer.  
\end{itemize}

\begin{table}[!ht]
  \centering
  \begin{tabular}{ |p{2cm}||p{3cm}| }
    \hline
    Tests set & Number of tests \\
    \hline
    FAQv5     & 2054            \\
    FAQ50     & 561             \\
    FAQ76     & 2025            \\
    FAQ76v2   & 1965            \\
    \hline
    \hline
    All       & 6605            \\
    \hline
  \end{tabular}
  \caption{Information about UPV FAQ tests} \label{table:UPV_FAQ_info}
\end{table}


\subsection{Baseline}
Due to the inherent morphological richness of the Czech language, this study adopts \textbf{FastText} as the baseline word embedding method.
This decision is motivated by FastText's ability to effectively capture morphological variations within words, a characteristic that has been shown to be advantageous for languages like Czech.
While other techniques like Word2Vec and \ac{GloVe} have been explored for word embedding generation, they have demonstrated lower performance in this context.
The FastText word embedding model will be trained using the fastText.cc library \footnote{fastText.cc library website: \url{https://fasttext.cc/}}

\subsubsection{Training parameters}
\begin{itemize}
  \item Architecture: \ac{CBOW} 
  \item Vector dimensionality: 300
  \item Loss function: Negative sampling loss
  \item Dictionary threshold (the frequency of the word to be included in the dictionary): 130
\end{itemize}

\subsubsection{Training data}

FastText model is trained on a segment of a preprocessed Common Crawl repository, which encompasses raw data from web pages, as well as metadata and text extractions.
Given the nature of this dataset, it is expected to include misspellings and text lacking diacritics.

\begin{enumerate}
  \item Eliminating duplicate entries
  \item Filtering out lines containing fewer than 9 characters
  \item Excluding URLs
  \item Breaking lines into individual sentences
  \item Converting all text to lowercase
  \item Removing lines containing words exceeding 30 characters
  \item Excluding HTML tags with less than 100 characters
  \item Removing lines surpassing 500 characters
  \item Omitting words longer than 21 characters
\end{enumerate}

Processed corpus contains 3.87 billion words.

\subsubsection{Model summary}

The FastText models trained in this study exhibit differences in vocabulary size.
The diacritic model possesses a dictionary of approximately 800,000 words, while the diacriticless model contains roughly 762,000 words.
This discrepancy reflects the inherent reduction in word count due to the removal of diacritics in the diacriticless dataset.

These vocabulary sizes directly influence the number of trainable parameters within each model.
The number of parameters ($N$) can be calculated using the formula~\refeq{eq:cbow_parameters}

\begin{align} \label{eq:cbow_parameters}
  & N = V_{\text{size}}d + d \\
  \text{where} \quad & V_{\text{size}} \text{ is model vocabulary size}  \notag \\
                      & d \quad \text{is chosen dimensionality of word embeddings} \notag
\end{align}
  
Applying this formula to the vocabulary sizes of our models:
\begin{itemize}
  \item The diacritics model possesses approximately 240 million parameters.
  \item The diacriticless model has approximately 229 million parameters.
\end{itemize}

\subsection{Chosen transformer models}
To ensure a comprehensive evaluation, a curated selection of embedding models will be utilized.
This selection encompasses three distinct categories:

\begin{enumerate}
  \item \textbf{Existing Czech Embedding Models}: This category incorporates established Czech embedding models developed within the Czech NLP community. Their inclusion allows for a focused analysis of how these models perform specifically for the Czech language.
  \item \textbf{Multilingual Models from the \ac{MTEB}}: The evaluation will leverage highly regarded multilingual models readily available through the \ac{MTEB}. This inclusion enables an assessment of how these models generalize to the Czech language, providing insights into their adaptability across languages.
  \item \textbf{Popular Monolingual Models from the \ac{MTEB} (rank is lower than 100)}: In addition to multilingual models, this selection will also include well-regarded monolingual models (models trained on a single language) from the \ac{MTEB}. This allows for a comparative analysis of how these models, potentially trained on English or other high-resource languages, perform on the Czech dataset.
\end{enumerate}

All models should be open-source. And sentence-bert. <1B. With information about training process.
If exisit two version of models - monolingual (no czech lang) and multilingual, we evaluate only multilingual

% Czech models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Czert
\subsubsection{Czert-B} \label{modelczert-b}

The Czert model \cite{czert} is a set of Czech \ac{BERT}-like language representation models developed specifically to enhance performance in processing the Czech language.
These models leverage the \ac{BERT} and \ac{ALBERT} \cite{ALBERT} architectures and are designed to outperform multilingual models by training exclusively on Czech data.
The training set includes a comprehensive corpus of Czech texts, such as Wikipedia articles, news, and other texts, accumulating to around 36GB of data.

There are 2 variants of the Czert model, Czert-A and Czert-B. Unfortunately Czert-A model is not available, so we will test only Czert-B model.
Czert-B model is based on the traditional \ac{BERT} architecture (110M parameters).
Model are pre-trained from scratch using \ac{MLM} and \ac{NSP} tasks. However, a slight modification is made to the \ac{NSP} task to adapt it better to the Czech language corpus structure.

% Seznam
\subsubsection{Seznam's models} \label{model:seznam}
This study leverages a group of compact word embedding models specifically designed for the Czech language.
These models were developed by the Seznam research group with a focus on efficient word representation generation \cite{seznam-models}.

\begin{itemize}
  \item RetroMAE-Small: \ac{BERT}-small model trained with the \ac{RetroMAE} objective \cite{RetroMAE} on a custom Czech corpus.
  \item Dist-MPNet-ParaCrawl: \ac{BERT}-small model distilled from the \textit{sentence-transformers/all-mpnet-base-v2} \footnote{\label{hf_all-mpnet-base-v2} Model on the huggingface website: \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}} model, using parallel cs-en dataset  \cite{ParaCrawl} for training.
  \item Dist-MPNet-CzEng: \ac{BERT}-small model distilled from the \textit{sentence-transformers/all-mpnet-base-v2} \footref{hf_all-mpnet-base-v2} . model, using parallel cs-en dataset Czeng \cite{CzEng_dataset} for training.
  \item SimCSE-RetroMAE-Small: The \ac{RetroMAE}-Small model fine-tuned with the \ac{SimCSE} \cite{SimCSE}.
  \item SimCSE-Dist-MPNet-ParaCrawl: The Dist-MPNet-ParaCrawl model fine-tuned with the \ac{SimCSE}.
  \item SimCSE-Dist-MPNet-CzEng: The Dist-MPNet-CzEng fine-tuned with the \ac{SimCSE}.
  \item SimCSE-Small-E-Czech: Czech ELECTRA model \cite{small-e-czech} fine-tuned with the \ac{SimCSE} objective to enhance sentence embeddings.
\end{itemize}

The developed models are about eight times smaller and five times faster than conventional base-sized models, making them suitable for real-time applications where computational efficiency is critical.
The models are trained using techniques like pre-training, knowledge distillation, and unsupervised contrastive fine-tuning to adapt to the limited availability of labeled Czech data.


% Multilingual models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% mBERT
\subsubsection{mBERT} \label{model:mbert}
\ac{mBERT} \cite{bert_mbert} leverages the same transformer-based architecture as the original \ac{BERT} model but boasts an increased number of parameters (178M) to enhance its capabilities.
A key distinction of \ac{mBERT} lies in its ability to understand and process text data across multiple languages.
To achieve this multilingual proficiency, \ac{mBERT} is trained on a massive dataset sourced from Wikipedia entries in 104 different languages.
This corpus is meticulously constructed to ensure balanced representation, meaning each language is included regardless of the size or depth of its corresponding Wikipedia.
This approach ensures that even languages with limited resources are adequately represented within the training data, fostering better performance for these languages.

% mE5
\subsubsection{\ac{mE5}} \label{model:me5}
\ac{mE5} models by Microsoft \cite{mE5} are advanced text embedding models designed to operate across multiple languages based on English-only E5 models \cite{E5}.
These models are available in three variants — small, base, and large — catering to different computational efficiency and performance needs.
The \ac{mE5} models are trained using a two-phase approach.
The first phase involves weakly-supervised contrastive pre-training on about 1 billion text pairs sourced from diverse multilingual corpora (Wikipedia, mC4, Multilingual CC News, Reddit, etc.).
The second phase is supervised fine-tuning on approximately 1.6 million data points from high-quality labeled datasets (MS MARCO \cite{ms_marco_dataset}, Natural Questions (NQ) \cite{NQ_dataset}, TriviaQA \cite{TriviaQA}, SQuAD \cite{SQuAD}, etc.).

% LaBSE
\subsubsection{LaBSE} \label{model:labse}
\ac{LaBSE} model \cite{LaBSE}, developed by Google, is a state-of-the-art model for generating sentence embeddings that are effective across 109 languages.
It leverages the transformer architecture and is trained on both monolingual (from sources like CommonCrawl and Wikipedia) and bilingual data (mined from web pages).
LaBSE utilizes a dual-encoder structure with \ac{BERT}-based encoding modules.
This setup enables the efficient processing of text pairs in multiple languages.

% XLM-R
\subsubsection{XLM-R} \label{model:xlm-roberta}
The \ac{XLM-R} model \cite{xlm-roberta} is a significant advancement in unsupervised cross-lingual representation learning, introduced by Facebook AI.
It is specifically designed to improve performance across a wide range of cross-lingual tasks.
\ac{XLM-R} is pre-trained on a dataset dubbed 'CC-100', derived from Common Crawl, covering about 2.5 terabytes of text across 100 languages.
This dataset is significantly larger than the ones used by its predecessors, offering a broader and more diverse linguistic foundation.

% SentenceTransformers
\subsubsection{SentenceTransformers models \cite{reimers-2019-sentence-bert}} \label{model:st-multilingual}

\begin{itemize}
  \item \textbf{distiluse-base-multilingual-cased-v2}: This model leverages knowledge distillation, a technique for creating a smaller and faster model by capturing the knowledge from a larger, pre-trained model. In this case, it is a distilled version of the multilingual Universal Sentence Encoder \cite{yang2019multilingual}. Notably, this version supports sentence encoding for over 50 languages, including Czech.
  \item \textbf{paraphrase-multilingual-MiniLM-L12-v2}: This pre-trained model focuses on paraphrase identification. It is a multilingual version of the paraphrase-MiniLM-L12-v2 model, trained on parallel datasets encompassing over 50 languages, including Czech. By learning paraphrase relationships, this model can potentially capture semantic similarities between sentences.
  \item \textbf{paraphrase-multilingual-mpnet-base-v2}: Similar to the previous model, this is a multilingual version of the paraphrase-mpnet-base-v2 model, trained on parallel data for over 50 languages, including Czech.  This model is also designed for paraphrase identification, potentially aiding in tasks that require an understanding of semantic equivalence across sentences.
\end{itemize}


% Monolingual models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% UAE-Large-V1
\subsubsection{UAE-Large-V1} \label{model-uae-large-v1}
The UAE-Large-V1 model \cite{li2023angle} focuses on enhancing short and long \ac{STS} tasks through a novel angle-optimized text embedding approach (AnglE) that works by dividing text embeddings into real and imaginary components in a complex space.
This model is designed to address the challenges posed by the saturation zones of the cosine function, which can impede learning by causing vanishing gradients.

The model is trained using a hybrid objective that combines cosine similarity, in-batch negatives, and angle differences in complex space.
This approach helps overcome the limitations of traditional cosine similarity measures by ensuring better gradient flow during training.
Training dataset includes around 21K samples and is specifically designed to evaluate \ac{STS} performance on long texts, which are common in real-world applications.

% Nomic Embed
\subsubsection{Nomic-Embed-v1 and Nomic-Embed-v1.5} \label{model:nomic-embed}
The Nomic model \cite{nussbaum2024nomic}, designed for reproducible long-context text embedding, employs a modified BERT architecture optimized for 8192 token sequences.
It includes innovations such as rotary positional embeddings and SwiGLU activations, enhancing its ability to process longer texts.
The model undergoes a comprehensive training regime starting with unsupervised contrastive pretraining on large-scale datasets, followed by supervised fine-tuning using human-annotated data.
This process leverages contrastive learning to improve text embedding capabilities, significantly enhancing performance on both short and long-context tasks.

Nomic Embed V1.5 is an improvement upon Nomic Embed V1 that utilizes Matryoshka Representation Learning \cite{kusupati2024matryoshka} which gives developers the flexibility to trade off the embedding size for a negligible reduction in performance.

% snowflake-arctic-embed
\subsubsection{snowflake-arctic-embed} \label{model:snowflake-embed}
This study investigates the use of snowflake-arctic-embed, a suite of text embedding models optimized for retrieval tasks. The suite offers multiple model sizes (xs, s, m, m-long, l) for flexibility.

Snowflake-arctic-embed leverages a two-stage training pipeline
\begin{enumerate}
  \item Pretraining on large batches of query-document pairs with dynamically generated negative examplesusing dataset with 400M samples combining public data and proprietary web search data.
  \item Fine-tuning the model on a smaller dataset (1M samples) consisting of query-positive document-negative document triplets to improve retrieval accuracy.
\end{enumerate}

% GTE
\subsubsection{GTE and GTE-v1.5} \label{model:gte}
The \ac{GTE} model is a pivotal development in \ac{NLP}, utilizing a deep Transformer encoder based on a \ac{BERT}-like architecture for generating dense text embeddings.
Initially, the \ac{GTE} model is unsupervisedly pre-trained on approximately 800 million text pairs from diverse web sources, enabling broad semantic coverage.
It then undergoes supervised fine-tuning with 3 million annotated text triples from varied datasets, including MS MARCO and Natural Questions, applying contrastive learning to enhance text relevance detection and similarity assessments.
This dual-stage training strategy equips the \ac{GTE} model to excel in complex \ac{NLP} tasks, demonstrating significant versatility and robust performance across multiple applications.

There is also updated version \ac{GTE}-v1.5.

% BGE
\subsubsection{BGE-v1.5} \label{model:bge}
\ac{BGE} Models \cite{bge_embedding} are built on a BERT-like architecture, available in three sizes: small, base, and large.
They are trained using a sophisticated multi-stage process involving pre-training on large unlabeled data, contrastive learning for fine-tuning on text pairs, and multi-task learning with high-quality labeled datasets.
This training regimen equips BGE models to handle a wide range of text embedding tasks with high efficiency and accuracy.


% TaylorAI
\subsubsection{TaylorAI tiny models} \label{model:taylorai}
This study incorporates two distilled transformer models from TaylorAI for evaluation:

\begin{itemize}
  \item \textbf{TaylorAI/BGE-micro-v2} \footnote{TaylorAI/bge-micro-v2 model on the huggingface website: \url{https://huggingface.co/TaylorAI/bge-micro-v2}}: This model is a 2-step distilled version of the small BGE-v1.5 model
  \item \textbf{TaylorAI/GTE-tiny} \footnote{TaylorAI/gte-tiny model on the huggingface website: \url{https://huggingface.co/TaylorAI/gte-tiny}}: This model is a distilled version of the small version of GTE model
\end{itemize}

\subsubsection{Mxbai embed}
This is study incorporates two models developed by MixedBread AI, mxbai-embed-large-v1 and mxbai-embed-2d-large-v1. 
\begin{itemize}
  \item \textbf{mxbai-embed-large-v1} \cite{emb2024mxbai}: This model stands out as a high-performance English embedding model specifically designed for \ac{RAG} systems.  As of March 2024, it holds the leading position among publicly available models of its class within the \ac{MTEB}. Notably, mxbai-embed-large-v1 surpasses other models in tasks such as classification, clustering, and retrieval. The success of this model can be attributed to its robust training methodology. Mxbai-embed-large-v1 is trained on a massive dataset exceeding 700 million text pairs using a contrastive learning approach. This approach focuses on maximizing the similarity between semantically similar texts while contrasting dissimilar ones. Furthermore, the model undergoes fine-tuning with 30 million high-quality triplets leveraging the AnglE loss function. This fine-tuning step further refines the model's ability to distinguish semantic relationships within text data
  \item \textbf{mxbai-embed-2d-large-v1} \cite{emb2024mxbai2d}: This model introduces a novel 2D-Matryoshka architecture \cite{li20242d}, marking a significant advancement in the field of text embedding. The 2D-Matryoshka architecture offers a key advantage over traditional approaches: it allows for both dimensionality reduction of embeddings and chunking of model layers. This  flexibility enables users to tailor the model size and complexity based on their specific computational needs. This allows for a crucial trade-off between computational efficiency and accuracy in resource-constrained environments. The model was designed to address the limitations of traditional dense embedding models, which produce fixed-size embeddings. These fixed-size embeddings can be inefficient for tasks requiring rapid processing or limited memory footprints. Mxbai-embed-2d-large-v1 tackles this challenge by employing a novel training strategy. This strategy incorporates contrastive training on a diverse dataset and fine-tuning on high-quality triplets. This approach allows the model to achieve competitive performance while offering significant reductions in resource consumption compared to traditional dense models.
\end{itemize}


\subsection{ember-v1}
This study leverages the ember-v1 model, developed by LLMRails.
Ember-v1 is a text embedding model trained on a comprehensive dataset of text pairs encompassing a wide range of domains such as finance, science, medicine, law, and beyond.
Notably, the training process incorporates techniques inspired by both RetroMAE \cite{RetroMAE} and SetFit \cite{tunstall2022efficient}. 

\subsection{GIST-Embedding-v0}
This study incorporates the GIST-Embedding-v0 suite of models, developed using the "Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning" (GIST) technique \cite{solatorio2024gistembed}. These models leverage pre-trained models as a foundation and are then fine-tuned on specific datasets.

The fine-tuning process for GIST-Embedding-v0 utilizes the MEDI datasets \cite{su2023embedder}, which are further enhanced by the inclusion of mined triplets derived from the \ac{MTEB} Classification training dataset. This targeted augmentation strategy aims to improve the model's performance on specific tasks.

Based on their performance in the \ac{MTEB}, our evaluation will focus solely on the GIST-Embedding-v0 models built upon the \ac{BGE}-v1.5 architecture (small, base, and large sizes).
This selection ensures we investigate the most promising fine-tuning approaches within the GIST-Embedding suite.







% TODO:
% \subsubsection{jina-embeddings}
%Only jina-embeddings-v2, because it's better in \ac{MTEB}
%
%\subsection{mxbai-embed-large-v1}

\section{Optimizing Text Representations for RAG in Technical QA}

\subsection{Key factors}
To identify the most suitable text representation model for \ac{RAG} in technical \ac{QA}, we propose a comprehensive evaluation approach that considers the following key factors:

\begin{itemize}
  \item \textbf{Embedding Efficiency}: Computational efficiency is crucial for real-world applications. We will evaluate the processing time required for different representation models, considering factors like embedding dimensionality and model complexity. This ensures the chosen model can handle real-time \ac{QA} tasks within reasonable processing time constraints.
  \item \textbf{Text Chunk Size}: The size of text chunks used by the \ac{RAG} algorithm (e.g., words, sentences, or paragraphs) can impact performance. We will investigate the optimal chunk size for technical \ac{QA} tasks. Here, we will balance the granularity of information retrieved by the model with computational efficiency. Smaller chunks (words) might capture finer details but require more processing, while larger chunks (paragraphs) might be faster to process but might miss relevant details.  
  \item \textbf{Factuality of Generated Answers}:  The primary objective is to generate answers that are factually accurate and consistent with the technical document. We will evaluate the models based on metrics that assess the factual correctness and coherence of the generated answers in response to technical questions. This ensures the generated answers are reliable and trustworthy for the user.
\end{itemize}


\section{REMOVE: Methodology structure}

\begin{itemize}
    \item Describe the evaluation process for different text representations.
      \begin{itemize}
        \item{Specify the chosen word, sentence, and paragraph representation models (e.g., FastText, BERT variants).}
        \item{Explain the usage of analogy tests and confusion matrices for evaluation.}
        \item{Detail the selection process for the UPV corpus set and its suitability for technical QA tasks.}
      \end{itemize}
    \item Outline the second part of the study focusing on RAG for technical QA.
      \begin{itemize}
        \item{Explain the RAG algorithm and its reliance on text representations.}
        \item{Describe the evaluation approach for selecting optimal representations for RAG.}
        \item{Mention the factors considered during evaluation, such as embedding efficiency, text chunk size, and factuality of generated answers.}
      \end{itemize}
\end{itemize}









