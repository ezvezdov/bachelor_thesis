% Word2Vec
@misc{word2vec,
  title         = {Efficient Estimation of Word Representations in Vector Space},
  author        = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  year          = {2013},
  eprint        = {1301.3781},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

% GloVe
@inproceedings{glove,
  author    = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {GloVe: Global Vectors for Word Representation},
  year      = {2014},
  pages     = {1532--1543},
  url       = {http://www.aclweb.org/anthology/D14-1162}
}

% FastText
@article{fasttext,
  title   = {Enriching Word Vectors with Subword Information},
  author  = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal = {arXiv preprint arXiv:1607.04606},
  year    = {2016}
}

% Transformers
@misc{vaswani2023attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

% MTEB
@misc{mteb,
  title         = {MTEB: Massive Text Embedding Benchmark},
  author        = {Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
  year          = {2023},
  eprint        = {2210.07316},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{bert_mbert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{czert,
  author       = {Jakub Sido and
                  Ondrej Praz{\'{a}}k and
                  Pavel Prib{\'{a}}n and
                  Jan Pasek and
                  Michal Sej{\'{a}}k and
                  Miloslav Konop{\'{\i}}k},
  title        = {Czert - Czech BERT-like Model for Language Representation},
  journal      = {CoRR},
  volume       = {abs/2103.13031},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.13031},
  eprinttype    = {arXiv},
  eprint       = {2103.13031},
  timestamp    = {Thu, 14 Oct 2021 09:16:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-13031.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{seznam-models,
      title={Some Like It Small: Czech Semantic Embedding Models for Industry Applications}, 
      author={Jiří Bednář and Jakub Náplava and Petra Barančíková and Ondřej Lisický},
      year={2023},
      eprint={2311.13921},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{mE5,
  title         = {Multilingual E5 Text Embeddings: A Technical Report},
  author        = {Liang Wang and Nan Yang and Xiaolong Huang and Linjun Yang and Rangan Majumder and Furu Wei},
  year          = {2024},
  eprint        = {2402.05672},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{E5,
      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2212.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{LaBSE,
      title={Language-agnostic BERT Sentence Embedding}, 
      author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},
      year={2022},
      eprint={2007.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% AnglE
@article{li2023angle,
  title={AnglE-optimized Text Embeddings},
  author={Li, Xianming and Li, Jing},
  journal={arXiv preprint arXiv:2309.12871},
  year={2023}
}

@article{xlm-roberta,
  author       = {Alexis Conneau and
                  Kartikay Khandelwal and
                  Naman Goyal and
                  Vishrav Chaudhary and
                  Guillaume Wenzek and
                  Francisco Guzm{\'{a}}n and
                  Edouard Grave and
                  Myle Ott and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal      = {CoRR},
  volume       = {abs/1911.02116},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.02116},
  eprinttype    = {arXiv},
  eprint       = {1911.02116},
  timestamp    = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ALBERT,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{RetroMAE,
      title={RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder}, 
      author={Shitao Xiao and Zheng Liu and Yingxia Shao and Zhao Cao},
      year={2022},
      eprint={2205.12035},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ParaCrawl,
  title={ParaCrawl: Web-scale parallel corpora for the languages of the EU},
  author={Esplà, Miquel and Forcada, Mikel and Ramírez-Sánchez, Gema and Hoang, Hieu},
  booktitle={Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks},
  pages={118--119},
  year={2019},
  organization={European Association for Machine Translation}
}

@misc{CzEng_dataset,
      title={Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords}, 
      author={Tom Kocmi and Martin Popel and Ondrej Bojar},
      year={2020},
      eprint={2007.03006},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{SimCSE,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={6894--6910},
  year={2021}
}

@misc{small-e-czech,
      title={Siamese BERT-based Model for Web Search Relevance Ranking Evaluated on a New Czech Dataset}, 
      author={Matěj Kocián and Jakub Náplava and Daniel Štancl and Vladimír Kadlec},
      year={2021},
      eprint={2112.01810},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{ms_marco_dataset,
      title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset}, 
      author={Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara and Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
      year={2018},
      eprint={1611.09268},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{NQ_dataset,
  title={Natural Questions: A Benchmark for Question Answering Research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion},
  journal={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3877--3886},
  year={2019}
}

@article{TriviaQA,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{SQuAD,
  title={Squad: A Question Answering Dataset for Readers},
  author={Rajpurkar, Pranav and Jia, Robin and Polosukhin, Ilya},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

% Multilingual Universal Sentence Encoder for Semantic Retrieval
@misc{yang2019multilingual,
      title={Multilingual Universal Sentence Encoder for Semantic Retrieval}, 
      author={Yinfei Yang and Daniel Cer and Amin Ahmad and Mandy Guo and Jax Law and Noah Constant and Gustavo Hernandez Abrego and Steve Yuan and Chris Tar and Yun-Hsuan Sung and Brian Strope and Ray Kurzweil},
      year={2019},
      eprint={1907.04307},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% SentenceTransformers models
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
}

% BGE
@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% nomic-embed-text-v1
@misc{nussbaum2024nomic,
      title={Nomic Embed: Training a Reproducible Long Context Text Embedder}, 
      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},
      year={2024},
      eprint={2402.01613},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Matryoshka Representation Learning
@misc{kusupati2024matryoshka,
      title={Matryoshka Representation Learning}, 
      author={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},
      year={2024},
      eprint={2205.13147},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Retrieval-Augmented Generation
@misc{lewis2021retrievalaugmented,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% mxbai-embed-large-v1
@online{emb2024mxbai,
  title={Open Source Strikes Bread - New Fluffy Embeddings Model},
  author={Sean Lee, Aamir Shakir, Darius Koenig, Julius Lipp},
  year={2024},
  url={https://www.mixedbread.ai/blog/mxbai-embed-large-v1},
}

% mxbai-embed-2d-large-v1
@online{emb2024mxbai2d,
  title={Fresh 2D-Matryoshka Embedding Model},
  author={Sean Lee, Aamir Shakir, Julius Lipp, Darius Koenig},
  year={2024},
  url={https://www.mixedbread.ai/blog/mxbai-embed-2d-large-v1},
}

% 2D Matryoshka Sentence Embeddings
@misc{li20242d,
      title={2D Matryoshka Sentence Embeddings}, 
      author={Xianming Li and Zongxi Li and Jing Li and Haoran Xie and Qing Li},
      year={2024},
      eprint={2402.14776},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% SetFit
@misc{tunstall2022efficient,
      title={Efficient Few-Shot Learning Without Prompts}, 
      author={Lewis Tunstall and Nils Reimers and Unso Eun Seo Jo and Luke Bates and Daniel Korat and Moshe Wasserblat and Oren Pereg},
      year={2022},
      eprint={2209.11055},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% MEDI dataset
@misc{su2023embedder,
      title={One Embedder, Any Task: Instruction-Finetuned Text Embeddings}, 
      author={Hongjin Su and Weijia Shi and Jungo Kasai and Yizhong Wang and Yushi Hu and Mari Ostendorf and Wen-tau Yih and Noah A. Smith and Luke Zettlemoyer and Tao Yu},
      year={2023},
      eprint={2212.09741},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% GIST Embedding v0
@misc{solatorio2024gistembed,
      title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}, 
      author={Aivin V. Solatorio},
      year={2024},
      eprint={2402.16829},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}